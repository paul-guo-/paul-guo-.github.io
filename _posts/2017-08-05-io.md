---
layout: post
title:  "Linux系统IO浅谈"
author: Paul Guo
date:   2017-07-30
published: true
---

*这篇文章以前发表于微信公众号*

## Linux系统IO简介

我们知道传统UNIX操作系统把运行时代码数据分用户层（用户态）和内核层（内核态）。内核态更多偏向于硬件资源（CPU、IO等）的控制层面代码，内核提供所谓的系统调用（system call）供用户态程序使用以达到安全隔离的目的。应用对硬件存储系统的访问也不例外，除了很少情况一些特殊软件能直接在用户层面类似于内核驱动程序那样操作本地磁盘，大部分时候我们还是使用传统的open(), read(), write(), close()等系统调用来直接的（比如直接读取磁盘分区）或者间接的（通过访问文件系统）访问存储系统。后面我们以文件系统方式访问存储系统为例展开Linux IO的讨论。我们知道传统机械磁盘对比内存CPU，访问速度差距明显，而且相比顺序读写、其随机读写性能特别差（因为需要机械寻道），长久以来Linux操作系统对此做了大量的优化。
 
## Page Cache

Page cache是Linux内核一个针对磁盘IO访问优化的中间内存缓存层，page cache 存在于物理内存上。通常来说，应用在读取文件的时候会首先在page cache中查找相应文件内容（内存页面）是否存在，如果存在就会使用page cache中的数据，而如果不存在的时候内核会把数据装载到物理内存（page cache）中，而写入的时候也会先写入到page cache中，这些已经修改或者磁盘中内容不存在的内存页面（统称为脏页） 需要最后同步到磁盘以防止突然断电或者系统崩溃等异常造成的数据丢失。某个文件在内核中的page cache不会随着close()的系统调用或者进程退出被马上回收或者释放掉，即使这个时候没有别的进程在读写访问这个文件。这也就是为什么我们有时候在Linux上打开一个文件不久再次打开会明显变快。而我们有时候在Linux使用free命令查看系统内存状况显示free项很小，其实不代表可用内存很少，因为通常有很大一部分是page cache，而这些page cache通常是比较容易回收的。
 
Linux内核有个参数可以间接控制page cache的大小：vm.min_free_kbytes
这个参数其实还比较重要，因为虽然Page cache是可以被回收的，但是在某些情况下（比如内核处于所谓的atomic context的时候如果内存不够是回收不了的），如果内存或者满足某些条件的内存（比如物理地址连续）的缺失容易造成内存分配失败。所以 这个值也不能太小。
另外还有一个比较重要的参数：vm.swappiness 
[1]中的解释如下：

*This control is used to define how aggressive the kernel will swap memory pages. Higher values will increase agressiveness, lower values decrease the amount of swap. A value of 0 instructs the kernel not to initiate swap until the amount of free and file-backed pages is less than the high water mark in a zone.*
 
这个参数的使用不是那么直观，更多的是表达增加可用物理内存偏向所谓swap出去还是回收page cache以及偏向的度。对于物理内存通常很充足的数据库应用，这个参数可以调的比较小。

## 存储数据完整性保障

从Linux的角度来说，数据同步到了磁盘上就代表数据已经安全了。当然从实际角度来说不一定，比如硬盘坏甚至写丢失等各种问题，比如如果RAID存储的话，RAID存储卡缓存flush出现问题。这些已经不属于本文的范畴了。业界已有各种解决办法，比如分布式存储、RAID存储或者专业存储设备等。
通常来说，Linux系统内核文件系统的访问是会通过page cache，而内核也会有线程通过一种机制控制脏页数据同步写回磁盘，Linux的策略是dirty超过了一定时间的页面必须得同步或者空闲页面少于一定比率的时候也会触发同步以便于页面回收，这种机制是很自然可以理解的。这种机制是自动的，对用户应用是透明的。
而Linux也提供了主动发起数据同步回磁盘的系统调用：sync()或者fsync()或者fdatasync()。 
```C
	void sync(void);
	int fsync(int fd);
	int fdatasync(int fd);  
```
其中：sync()是一个比较重的系统调用，因为是针对整个系统的，一般应用使用较少。fsync()是针对打开单个文件，在很多对数据完整性要求比较高的应用使用较多，比如很多数据库软件中我们会看到调用fsync()同步数据。fdatasync()和fsync()类似，都是针对的单个打开文件，唯一不同的时候它不同步文件元数据（metadata），一般元数据是指存在文件inode数据块中的数据，包含了文件各种时间戳、文件拥有者权限、文件大小等。如果某些元数据没有被同步可能会造成一些严重问题，比如文件大小的元数据，所以fdatasync()要谨慎的在某些特殊场景中使用。

Linux还有一个很有意思的系统调用sync_file_range() 。
```C
	int sync_file_range(int fd, off64_t offset, off64_t nbytes, unsigned int flags);
```
它做的事和fdatasync()类似的，但是从函数的参数来看，粒度更细。通常来说它会比fdatasync()更快，虽然它也不能更新文件的metadata，但是在合适的场景确实很有用。大量的系统软件比如postgresql/mysql/rocksdb等都有使用。

对于自动脏页同步写回，Linux内核也有各种参数来控制。典型的参数如下：
 
vm.dirty_background_ratio
 [1]中解释如下：

*Contains, as a percentage of total available memory that contains free pages and reclaimable pages, the number ofpages at which the background kernel flusher threads will start writing out dirty data.
The total available memory is not equal to total system memory.*
 
vm.dirty_expire_centisecs
 [1]中解释如下：

*This tunable is used to define when dirty data is old enough to be eligible for write out by the kernel flusher threads. It is expressed in 100'ths of a second. Data which has been dirty in-memory for longer than this interval will be written out next time a flusher thread wakes up.*
 
其实这两个参数就是一个调比率，一个调时间，很容易理解。关于脏页更多的参数可以参见[1].

其实除了上面提到的主动或者被动同步的方法，Linux在open()系统调用的时候也提供一种参数O_SYNC以确保每次write()返回的时候数据(包括metadata)已经写入磁盘。open系统调用的manpage中关于这个参数的解释如下：
 
O_SYNC

The file is opened for synchronous I/O. Any write(2) on the resulting file descriptor will block the calling process until the data has been physically written to the underlying hardware.

但是这种方式似乎性能很差，使用很少。

## 绕过page cache?
Linux在open()的时候也提供一种O_DIRECT的参数，能绕过page cache，这样每次调用write()的时候能保证数据已经写到磁盘，但是文件的metadata信息完整性不能保证，因此还是需要fsync()或者搭配O_SYNC来实现真正的数据完整性。不过在随机读写比较多得情况下，O_DIRECT可能会有不错的性能。使用O_DIRECT有个问题要注意，Linux内核的作者Linus曾经说过针对同样一个文件，要避免混用O_DIRECT和page cache情况，原因并非很容易想到的数据一致性问题，而是性能问题，具体最新的内核是否有这个问题或者原因是否这样有待于查看代码确认。

## 预读
我们知道存储中一个很基本的性能优化套路就是：预先读、延迟写。是的，Linux也提供了预读，以提高顺序读的性能。Linux同时也提供了主动预读和被动预读。主动预读是通过阻塞式的系统调用readahead()发起的：
```C
	ssize_t readahead(int fd, off64_t offset, size_t count);
```
被动预读对应用是透明的，用户没法直接控制，但是用户可以通过系统调用posix_fadvise()来更加细粒度的控制内核对某些文件数据的访问策略。
```C
	int posix_fadvise(int fd, off_toffset, off_t len, int advice);
```
比如我们可以对advice设置POSIX_FADV_SEQUENTIAL以提高内核预读的窗口（可以理解为更加激进的预读），也可以设置POSIX_FADV_WILLNEED以提示内核对相应的文件范围预读。
 
## Zero Copy
前面我们提到的经典文件访问都是通过内核读取写文件内容到page cache，然后应用层到内核层又会多一层拷贝，这样主要是因为安全。但是内存拷贝还是会很影响性能的，所以就有各种zero copy的方法想避免这种拷贝。

一种方法是系统调用mmap()
```C
	void* mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);
```
 
mmap()是一种比较重，也不是那么好理解的一种系统调用。针对文件的映射，大体上可以这么理解，我们可以把文件做个映射，建立虚拟地址空间，让用户可以通过类似内存访问那样直接读写文件内容。所谓的mmap()在内核看来是一件很容易理解的事情，我们知道现代操作系统都是通过虚拟地址访问物理内存，而虚拟地址和物理地址映射需要内核设置好页表，如果一个应用的虚拟地址在页表中没有对应的那一项，在访问的时候就是段错误。内核对文件的mmap实际上也是设置好页表映射关系，在应用访问某个文件区域的时候，如果内核发现无此内容就发出缺页中断(page fault)，然后就会把数据装载到内存中并在页表中设置好虚拟地址和物理地址的映射。因为已有地址映射，所以对文件操作避免了应用层和内核层的拷贝。
 
同样的，针对mmap的IO，也有类似前面的系统调用(当然没有那么全)，比如msync()、madvise()等。
 
mmap()比如read()/write()有那么多优点，但是也不是没有缺点的，比如：mmap()比如read()等需要更大的初始化代价所以可能有些时候小文件等的读写或者一小段文件临时读取并不合适，另外[2]中提到如果另外的进程在截断这个文件，而我们又在读取被截断部分，程序会异常退出。
 
另外一种系统调用是：sendfile()和splice()以及内核4.5才加入的copy_file_rage()。
```C
	ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);

	long splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out,
				size_t len, unsigned int flags);

	ssize_t copy_file_range(int fd_in, loff_t *off_in, int fd_out,loff_t *off_out,
				size_t len,unsigned int flags);
```
  
sendfile()和splice()主要用于不同文件之间的内核级别的拷贝，避免经由用户态绕一层。早先的sendfile()和splice()都对文件或者文件系统有一定的要求，比如2.6.33前sendfile()的out_fd必须是一个socket（其实从这个系统调用的名字上就可以理解。而很多web server也都有使用这个系统调用），但是2.6.33后不需要了。早先splice()要求必定有一个fd是pipe（因为其内核实现原理的要求）。两个系统调用早先其实交集有限，后来的sendfile()其实在内核的实现就是使用的splice()的代码（也就是说会使用per-process的的内部的一个pipe文件）。
 
copy_file_range()主要用于同文件系统的文件内容拷贝，利用了同样文件系统的拷贝加速、reference等功能，也是避免了应用层和内核层的拷贝。当然它也可以用于不同文件系统之间的文件内容拷贝。
 
## 总结
Linux IO系统作为和内存管理系统紧密关系的系统，非常复杂，本文覆盖了一些从用户角度比较关心的但是也很容易忽略的东西，很多更偏底层的东西比如IO调度、文件系统等没有涉猎。本文只是抛砖引玉，并没有深入内核细节讨论，因为Linux内核实现很复杂，变化很快，单单一项深入展看就会很大篇幅，而从大部分用户角度来说，本文的深度可能就足够用了。

## References
[1]. [Sysctl control for kernel vm](https://www.kernel.org/doc/Documentation/sysctl/vm.txt)

[2]. [Zero Copy I: User-Mode Perspective](http://www.linuxjournal.com/article/6345)
